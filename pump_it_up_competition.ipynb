{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Pump it Up: Data Mining the Water Table\n",
    "## DrivenData Competition - Water Pump Status Prediction\n",
    "\n",
    "### Objetivo del Negocio\n",
    "Predecir el estado funcional de bombas de agua en Tanzania para optimizar el mantenimiento y asegurar el acceso al agua potable. Este modelo ayudar√° a:\n",
    "- Identificar bombas que necesitan reparaci√≥n antes de fallar completamente\n",
    "- Optimizar recursos de mantenimiento\n",
    "- Mejorar la disponibilidad de agua para las comunidades\n",
    "\n",
    "### Clases a Predecir\n",
    "- **functional**: La bomba funciona correctamente\n",
    "- **non functional**: La bomba no funciona\n",
    "- **functional needs repair**: La bomba funciona pero necesita reparaci√≥n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Importaci√≥n de Librer√≠as y Configuraci√≥n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Librer√≠as b√°sicas\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# Configuraci√≥n de visualizaci√≥n\n",
    "plt.style.use('seaborn-v0_8-darkgrid')\n",
    "sns.set_palette('husl')\n",
    "%matplotlib inline\n",
    "\n",
    "# Librer√≠as de preprocesamiento\n",
    "from sklearn.model_selection import train_test_split, cross_val_score, StratifiedKFold\n",
    "from sklearn.preprocessing import LabelEncoder, StandardScaler\n",
    "from sklearn.impute import SimpleImputer\n",
    "\n",
    "# Modelos\n",
    "from sklearn.ensemble import RandomForestClassifier, GradientBoostingClassifier\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from xgboost import XGBClassifier\n",
    "from lightgbm import LGBMClassifier\n",
    "\n",
    "# M√©tricas\n",
    "from sklearn.metrics import accuracy_score, classification_report, confusion_matrix\n",
    "from sklearn.metrics import f1_score, precision_score, recall_score\n",
    "\n",
    "# Balanceo de clases\n",
    "from imblearn.over_sampling import SMOTE\n",
    "from collections import Counter\n",
    "\n",
    "print(\"‚úì Librer√≠as importadas correctamente\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Carga de Datos\n",
    "\n",
    "### Instrucciones para obtener los datos:\n",
    "1. Visitar: https://www.drivendata.org/competitions/7/pump-it-up-data-mining-the-water-table/\n",
    "2. Registrarse/Iniciar sesi√≥n\n",
    "3. Descargar los archivos:\n",
    "   - Training set values (train_values.csv)\n",
    "   - Training set labels (train_labels.csv)\n",
    "   - Test set values (test_values.csv)\n",
    "4. Colocar los archivos en el mismo directorio que este notebook"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cargar datos\n",
    "try:\n",
    "    train_values = pd.read_csv('train_values.csv')\n",
    "    train_labels = pd.read_csv('train_labels.csv')\n",
    "    test_values = pd.read_csv('test_values.csv')\n",
    "    \n",
    "    print(f\"‚úì Datos cargados exitosamente\")\n",
    "    print(f\"\\nDimensiones del dataset de entrenamiento: {train_values.shape}\")\n",
    "    print(f\"Dimensiones de las etiquetas: {train_labels.shape}\")\n",
    "    print(f\"Dimensiones del dataset de prueba: {test_values.shape}\")\n",
    "    \n",
    "    # Combinar datos de entrenamiento con etiquetas\n",
    "    train_data = train_values.merge(train_labels, on='id')\n",
    "    print(f\"\\nDataset completo de entrenamiento: {train_data.shape}\")\n",
    "    \n",
    "except FileNotFoundError as e:\n",
    "    print(f\"‚ùå Error: {e}\")\n",
    "    print(\"\\nPor favor, descarga los datos siguiendo las instrucciones anteriores.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Exploraci√≥n Inicial de Datos (EDA)\n",
    "\n",
    "### 3.1 Vista General del Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Informaci√≥n general\n",
    "print(\"=\" * 80)\n",
    "print(\"INFORMACI√ìN GENERAL DEL DATASET\")\n",
    "print(\"=\" * 80)\n",
    "print(f\"\\nN√∫mero total de bombas: {len(train_data):,}\")\n",
    "print(f\"N√∫mero de caracter√≠sticas: {train_data.shape[1] - 1}\")\n",
    "\n",
    "# Primeras filas\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"PRIMERAS FILAS DEL DATASET\")\n",
    "print(\"=\"*80)\n",
    "display(train_data.head())\n",
    "\n",
    "# Tipos de datos\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"TIPOS DE DATOS\")\n",
    "print(\"=\"*80)\n",
    "print(train_data.dtypes.value_counts())\n",
    "\n",
    "# Columnas\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"LISTA DE TODAS LAS CARACTER√çSTICAS\")\n",
    "print(\"=\"*80)\n",
    "for i, col in enumerate(train_data.columns, 1):\n",
    "    print(f\"{i:2d}. {col}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.2 An√°lisis de la Variable Objetivo\n",
    "\n",
    "**Insight de Negocio**: Entender la distribuci√≥n de estados es crucial para:\n",
    "- Identificar si existe desbalanceo de clases\n",
    "- Planificar estrategias de mantenimiento seg√∫n prevalencia\n",
    "- Ajustar modelos para manejar clases minoritarias"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Distribuci√≥n de la variable objetivo\n",
    "print(\"=\" * 80)\n",
    "print(\"DISTRIBUCI√ìN DE ESTADOS DE LAS BOMBAS\")\n",
    "print(\"=\" * 80)\n",
    "\n",
    "status_counts = train_data['status_group'].value_counts()\n",
    "status_percentages = train_data['status_group'].value_counts(normalize=True) * 100\n",
    "\n",
    "status_df = pd.DataFrame({\n",
    "    'Cantidad': status_counts,\n",
    "    'Porcentaje': status_percentages.round(2)\n",
    "})\n",
    "print(status_df)\n",
    "\n",
    "# Visualizaci√≥n\n",
    "fig, axes = plt.subplots(1, 2, figsize=(14, 5))\n",
    "\n",
    "# Gr√°fico de barras\n",
    "status_counts.plot(kind='bar', ax=axes[0], color=['green', 'red', 'orange'])\n",
    "axes[0].set_title('Distribuci√≥n de Estados de Bombas', fontsize=14, fontweight='bold')\n",
    "axes[0].set_xlabel('Estado')\n",
    "axes[0].set_ylabel('Cantidad')\n",
    "axes[0].tick_params(axis='x', rotation=45)\n",
    "\n",
    "# Gr√°fico de pastel\n",
    "axes[1].pie(status_counts, labels=status_counts.index, autopct='%1.1f%%', \n",
    "            colors=['green', 'red', 'orange'], startangle=90)\n",
    "axes[1].set_title('Proporci√≥n de Estados', fontsize=14, fontweight='bold')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(\"\\nüí° Insight: Se observa desbalanceo de clases, especialmente en 'functional needs repair'.\")\n",
    "print(\"   Ser√° necesario aplicar t√©cnicas de balanceo (SMOTE, class weights) en el modelado.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.3 An√°lisis de Valores Faltantes\n",
    "\n",
    "**Contexto de Negocio**: Los datos faltantes pueden indicar:\n",
    "- Falta de mantenimiento de registros\n",
    "- Bombas en √°reas remotas con poca supervisi√≥n\n",
    "- Necesidad de mejorar sistemas de informaci√≥n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# An√°lisis de valores faltantes y ceros\n",
    "print(\"=\" * 80)\n",
    "print(\"AN√ÅLISIS DE VALORES FALTANTES Y PROBLEM√ÅTICOS\")\n",
    "print(\"=\" * 80)\n",
    "\n",
    "missing_data = pd.DataFrame({\n",
    "    'Missing_Count': train_data.isnull().sum(),\n",
    "    'Missing_Percent': (train_data.isnull().sum() / len(train_data) * 100).round(2),\n",
    "    'Zeros_Count': (train_data == 0).sum(),\n",
    "    'Zeros_Percent': ((train_data == 0).sum() / len(train_data) * 100).round(2)\n",
    "})\n",
    "\n",
    "missing_data = missing_data[\n",
    "    (missing_data['Missing_Count'] > 0) | (missing_data['Zeros_Count'] > 0)\n",
    "].sort_values('Missing_Percent', ascending=False)\n",
    "\n",
    "if len(missing_data) > 0:\n",
    "    print(missing_data)\n",
    "    \n",
    "    # Visualizaci√≥n de valores faltantes\n",
    "    if missing_data['Missing_Percent'].sum() > 0:\n",
    "        top_missing = missing_data[missing_data['Missing_Percent'] > 0].head(10)\n",
    "        \n",
    "        plt.figure(figsize=(12, 6))\n",
    "        plt.barh(top_missing.index, top_missing['Missing_Percent'])\n",
    "        plt.xlabel('Porcentaje de Valores Faltantes')\n",
    "        plt.title('Top 10 Variables con Valores Faltantes', fontsize=14, fontweight='bold')\n",
    "        plt.tight_layout()\n",
    "        plt.show()\nelse:\n",
    "    print(\"‚úì No se detectaron valores faltantes expl√≠citos\")\n",
    "\n",
    "# Detectar valores problem√°ticos en variables espec√≠ficas\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"VALORES CERO O VAC√çOS EN VARIABLES CLAVE\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "# Coordenadas geogr√°ficas\n",
    "print(f\"\\nCoordenadas con valor 0:\")\n",
    "print(f\"  - longitude = 0: {(train_data['longitude'] == 0).sum():,}\")\n",
    "print(f\"  - latitude = 0: {(train_data['latitude'] == 0).sum():,}\")\n",
    "\n",
    "# Variables num√©ricas importantes\n",
    "numeric_cols = ['gps_height', 'population', 'construction_year']\n",
    "for col in numeric_cols:\n",
    "    if col in train_data.columns:\n",
    "        zero_count = (train_data[col] == 0).sum()\n",
    "        zero_pct = (zero_count / len(train_data) * 100)\n",
    "        print(f\"  - {col} = 0: {zero_count:,} ({zero_pct:.2f}%)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.4 An√°lisis de Variables Categ√≥ricas de Alta Cardinalidad\n",
    "\n",
    "**Desaf√≠o de Negocio**: Variables con muchas categor√≠as √∫nicas pueden:\n",
    "- Dificultar el entrenamiento de modelos\n",
    "- Requerir t√©cnicas especiales de encoding\n",
    "- Necesitar agrupaci√≥n de categor√≠as poco frecuentes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# An√°lisis de cardinalidad\n",
    "print(\"=\" * 80)\n",
    "print(\"AN√ÅLISIS DE CARDINALIDAD - VARIABLES CATEG√ìRICAS\")\n",
    "print(\"=\" * 80)\n",
    "\n",
    "categorical_cols = train_data.select_dtypes(include=['object']).columns.tolist()\n",
    "if 'status_group' in categorical_cols:\n",
    "    categorical_cols.remove('status_group')\n",
    "\n",
    "cardinality_data = []\n",
    "for col in categorical_cols:\n",
    "    unique_count = train_data[col].nunique()\n",
    "    cardinality_data.append({\n",
    "        'Variable': col,\n",
    "        'Categor√≠as_√önicas': unique_count,\n",
    "        'Muestra': ', '.join(train_data[col].value_counts().head(3).index.astype(str))\n",
    "    })\n",
    "\n",
    "cardinality_df = pd.DataFrame(cardinality_data).sort_values(\n",
    "    'Categor√≠as_√önicas', ascending=False\n",
    ")\n",
    "print(cardinality_df.to_string(index=False))\n",
    "\n",
    "# Clasificar por cardinalidad\n",
    "high_card = cardinality_df[cardinality_df['Categor√≠as_√önicas'] > 100]\n",
    "medium_card = cardinality_df[\n",
    "    (cardinality_df['Categor√≠as_√önicas'] > 10) & \n",
    "    (cardinality_df['Categor√≠as_√önicas'] <= 100)\n",
    "]\n",
    "low_card = cardinality_df[cardinality_df['Categor√≠as_√önicas'] <= 10]\n",
    "\n",
    "print(f\"\\nüìä Resumen de Cardinalidad:\")\n",
    "print(f\"   - Alta cardinalidad (>100): {len(high_card)} variables\")\n",
    "print(f\"   - Media cardinalidad (10-100): {len(medium_card)} variables\")\n",
    "print(f\"   - Baja cardinalidad (‚â§10): {len(low_card)} variables\")\n",
    "\n",
    "if len(high_card) > 0:\n",
    "    print(f\"\\nüí° Variables de alta cardinalidad requerir√°n:\")\n",
    "    print(f\"   - Target encoding o frequency encoding\")\n",
    "    print(f\"   - Agrupaci√≥n de categor√≠as raras\")\n",
    "    print(f\"   - Posible eliminaci√≥n si no son informativas\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.5 An√°lisis de Variables Num√©ricas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Estad√≠sticas descriptivas de variables num√©ricas\n",
    "numeric_features = train_data.select_dtypes(include=[np.number]).columns.tolist()\n",
    "if 'id' in numeric_features:\n",
    "    numeric_features.remove('id')\n",
    "\n",
    "print(\"=\" * 80)\n",
    "print(\"ESTAD√çSTICAS DESCRIPTIVAS - VARIABLES NUM√âRICAS\")\n",
    "print(\"=\" * 80)\n",
    "print(train_data[numeric_features].describe().round(2))\n",
    "\n",
    "# Visualizaci√≥n de distribuciones\n",
    "n_features = min(len(numeric_features), 9)\n",
    "fig, axes = plt.subplots(3, 3, figsize=(15, 12))\n",
    "axes = axes.flatten()\n",
    "\n",
    "for idx, col in enumerate(numeric_features[:n_features]):\n",
    "    train_data[col].hist(bins=50, ax=axes[idx], edgecolor='black')\n",
    "    axes[idx].set_title(f'Distribuci√≥n: {col}', fontweight='bold')\n",
    "    axes[idx].set_xlabel(col)\n",
    "    axes[idx].set_ylabel('Frecuencia')\n",
    "\n",
    "# Ocultar ejes vac√≠os\n",
    "for idx in range(n_features, 9):\n",
    "    axes[idx].set_visible(False)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Preprocesamiento y Feature Engineering\n",
    "\n",
    "### 4.1 Preparaci√≥n de Datos\n",
    "\n",
    "**Estrategia de Preprocesamiento**:\n",
    "1. Manejo de valores faltantes y ceros problem√°ticos\n",
    "2. Extracci√≥n de caracter√≠sticas temporales\n",
    "3. Encoding de variables categ√≥ricas\n",
    "4. Feature engineering basado en conocimiento del dominio"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess_data(df, is_training=True):\n",
    "    \"\"\"\n",
    "    Preprocesa el dataset aplicando limpieza y feature engineering.\n",
    "    \n",
    "    Args:\n",
    "        df: DataFrame a procesar\n",
    "        is_training: Si es True, incluye la variable objetivo\n",
    "    \n",
    "    Returns:\n",
    "        DataFrame procesado\n",
    "    \"\"\"\n",
    "    df = df.copy()\n",
    "    \n",
    "    print(\"Iniciando preprocesamiento...\")\n",
    "    \n",
    "    # 1. Manejo de coordenadas geogr√°ficas\n",
    "    print(\"  - Procesando coordenadas geogr√°ficas...\")\n",
    "    df['longitude'].replace(0, np.nan, inplace=True)\n",
    "    df['latitude'].replace(0, np.nan, inplace=True)\n",
    "    \n",
    "    # Imputar con mediana\n",
    "    df['longitude'].fillna(df['longitude'].median(), inplace=True)\n",
    "    df['latitude'].fillna(df['latitude'].median(), inplace=True)\n",
    "    \n",
    "    # 2. Manejo de construction_year\n",
    "    print(\"  - Procesando a√±o de construcci√≥n...\")\n",
    "    df['construction_year'].replace(0, np.nan, inplace=True)\n",
    "    df['construction_year'].fillna(df['construction_year'].median(), inplace=True)\n",
    "    \n",
    "    # Feature engineering: edad de la bomba\n",
    "    current_year = 2013  # A√±o aproximado de los datos\n",
    "    df['pump_age'] = current_year - df['construction_year']\n",
    "    df['pump_age'] = df['pump_age'].clip(lower=0)  # No edades negativas\n",
    "    \n",
    "    # 3. Manejo de population\n",
    "    print(\"  - Procesando poblaci√≥n...\")\n",
    "    df['population'].replace(0, np.nan, inplace=True)\n",
    "    df['population'].fillna(df['population'].median(), inplace=True)\n",
    "    df['has_population'] = (df['population'] > 0).astype(int)\n",
    "    \n",
    "    # 4. Manejo de gps_height\n",
    "    print(\"  - Procesando altura GPS...\")\n",
    "    df['gps_height'].replace(0, np.nan, inplace=True)\n",
    "    df['gps_height'].fillna(df['gps_height'].median(), inplace=True)\n",
    "    \n",
    "    # 5. Feature engineering geogr√°fico\n",
    "    print(\"  - Creando features geogr√°ficas...\")\n",
    "    # Distancia al centro aproximado de Tanzania\n",
    "    tanzania_center_lat = -6.369028\n",
    "    tanzania_center_lon = 34.888822\n",
    "    \n",
    "    df['distance_to_center'] = np.sqrt(\n",
    "        (df['latitude'] - tanzania_center_lat)**2 + \n",
    "        (df['longitude'] - tanzania_center_lon)**2\n",
    "    )\n",
    "    \n",
    "    # 6. Extracci√≥n de fecha\n",
    "    print(\"  - Procesando fecha de registro...\")\n",
    "    df['date_recorded'] = pd.to_datetime(df['date_recorded'])\n",
    "    df['year_recorded'] = df['date_recorded'].dt.year\n",
    "    df['month_recorded'] = df['date_recorded'].dt.month\n",
    "    df['day_of_year'] = df['date_recorded'].dt.dayofyear\n",
    "    \n",
    "    # Eliminar la columna original de fecha\n",
    "    df.drop('date_recorded', axis=1, inplace=True)\n",
    "    \n",
    "    # 7. Manejo de variables categ√≥ricas con valores vac√≠os\n",
    "    print(\"  - Procesando variables categ√≥ricas...\")\n",
    "    categorical_cols = df.select_dtypes(include=['object']).columns\n",
    "    \n",
    "    for col in categorical_cols:\n",
    "        if col != 'status_group' or not is_training:\n",
    "            # Reemplazar valores vac√≠os y NaN\n",
    "            df[col].fillna('unknown', inplace=True)\n",
    "            df[col].replace('', 'unknown', inplace=True)\n",
    "            df[col].replace(' ', 'unknown', inplace=True)\n",
    "    \n",
    "    # 8. Reducci√≥n de cardinalidad para variables categ√≥ricas\n",
    "    print(\"  - Reduciendo cardinalidad de variables categ√≥ricas...\")\n",
    "    high_cardinality_cols = ['installer', 'funder', 'wpt_name', 'subvillage', 'ward', 'scheme_name']\n",
    "    \n",
    "    for col in high_cardinality_cols:\n",
    "        if col in df.columns:\n",
    "            # Mantener top categor√≠as, agrupar el resto como 'other'\n",
    "            top_categories = df[col].value_counts().head(20).index\n",
    "            df[col] = df[col].apply(\n",
    "                lambda x: x if x in top_categories else 'other'\n",
    "            )\n",
    "    \n",
    "    print(\"‚úì Preprocesamiento completado\")\n",
    "    return df\n",
    "\n",
    "# Aplicar preprocesamiento\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"APLICANDO PREPROCESAMIENTO\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "train_processed = preprocess_data(train_data, is_training=True)\n",
    "test_processed = preprocess_data(test_values, is_training=False)\n",
    "\n",
    "print(f\"\\nDimensiones finales:\")\n",
    "print(f\"  - Train: {train_processed.shape}\")\n",
    "print(f\"  - Test: {test_processed.shape}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.2 Encoding de Variables Categ√≥ricas\n",
    "\n",
    "**Estrategia**: Usaremos Label Encoding para variables categ√≥ricas, ya que muchos algoritmos de √°rboles (Random Forest, XGBoost, LightGBM) lo manejan eficientemente."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def encode_features(train_df, test_df, target_col='status_group'):\n",
    "    \"\"\"\n",
    "    Codifica variables categ√≥ricas usando Label Encoding.\n",
    "    \n",
    "    Args:\n",
    "        train_df: DataFrame de entrenamiento\n",
    "        test_df: DataFrame de prueba\n",
    "        target_col: Nombre de la columna objetivo\n",
    "    \n",
    "    Returns:\n",
    "        train_encoded, test_encoded, label_encoders, target_encoder\n",
    "    \"\"\"\n",
    "    train_encoded = train_df.copy()\n",
    "    test_encoded = test_df.copy()\n",
    "    \n",
    "    # Guardar ID para submission\n",
    "    test_ids = test_encoded['id'].copy()\n",
    "    \n",
    "    # Identificar columnas categ√≥ricas\n",
    "    categorical_cols = train_encoded.select_dtypes(include=['object']).columns.tolist()\n",
    "    if target_col in categorical_cols:\n",
    "        categorical_cols.remove(target_col)\n",
    "    \n",
    "    print(\"=\" * 80)\n",
    "    print(\"CODIFICACI√ìN DE VARIABLES CATEG√ìRICAS\")\n",
    "    print(\"=\" * 80)\n",
    "    print(f\"\\nVariables a codificar: {len(categorical_cols)}\")\n",
    "    \n",
    "    # Diccionario para guardar encoders\n",
    "    label_encoders = {}\n",
    "    \n",
    "    # Codificar cada variable categ√≥rica\n",
    "    for col in categorical_cols:\n",
    "        le = LabelEncoder()\n",
    "        \n",
    "        # Combinar valores √∫nicos de train y test\n",
    "        combined_values = pd.concat([\n",
    "            train_encoded[col],\n",
    "            test_encoded[col]\n",
    "        ]).unique()\n",
    "        \n",
    "        # Fit en valores combinados\n",
    "        le.fit(combined_values)\n",
    "        \n",
    "        # Transform\n",
    "        train_encoded[col] = le.transform(train_encoded[col])\n",
    "        test_encoded[col] = le.transform(test_encoded[col])\n",
    "        \n",
    "        label_encoders[col] = le\n",
    "    \n",
    "    # Codificar variable objetivo\n",
    "    target_encoder = LabelEncoder()\n",
    "    train_encoded[target_col] = target_encoder.fit_transform(train_encoded[target_col])\n",
    "    \n",
    "    print(f\"\\n‚úì Codificaci√≥n completada\")\n",
    "    print(f\"\\nClases objetivo:\")\n",
    "    for idx, label in enumerate(target_encoder.classes_):\n",
    "        print(f\"  {idx}: {label}\")\n",
    "    \n",
    "    return train_encoded, test_encoded, label_encoders, target_encoder, test_ids\n",
    "\n",
    "# Aplicar encoding\n",
    "train_encoded, test_encoded, label_encoders, target_encoder, test_ids = encode_features(\n",
    "    train_processed, test_processed\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.3 Preparaci√≥n de Datasets para Modelado"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Separar caracter√≠sticas y objetivo\n",
    "X = train_encoded.drop(['status_group', 'id'], axis=1)\n",
    "y = train_encoded['status_group']\n",
    "\n",
    "# Preparar test set\n",
    "X_test_final = test_encoded.drop('id', axis=1)\n",
    "\n",
    "# Asegurar mismas columnas\n",
    "missing_cols = set(X.columns) - set(X_test_final.columns)\n",
    "for col in missing_cols:\n",
    "    X_test_final[col] = 0\n",
    "\n",
    "X_test_final = X_test_final[X.columns]\n",
    "\n",
    "print(\"=\" * 80)\n",
    "print(\"DATASETS FINALES PARA MODELADO\")\n",
    "print(\"=\" * 80)\n",
    "print(f\"\\nX (features): {X.shape}\")\n",
    "print(f\"y (target): {y.shape}\")\n",
    "print(f\"X_test_final: {X_test_final.shape}\")\n",
    "print(f\"\\nN√∫mero de caracter√≠sticas: {X.shape[1]}\")\n",
    "print(f\"\\nDistribuci√≥n de clases:\")\n",
    "print(y.value_counts())\n",
    "\n",
    "# Split train/validation\n",
    "X_train, X_val, y_train, y_val = train_test_split(\n",
    "    X, y, test_size=0.2, random_state=42, stratify=y\n",
    ")\n",
    "\n",
    "print(f\"\\nDivisi√≥n Train/Validation:\")\n",
    "print(f\"  - X_train: {X_train.shape}\")\n",
    "print(f\"  - X_val: {X_val.shape}\")\n",
    "print(f\"  - y_train: {y_train.shape}\")\n",
    "print(f\"  - y_val: {y_val.shape}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Entrenamiento de Modelos\n",
    "\n",
    "### 5.1 Configuraci√≥n de Modelos\n",
    "\n",
    "**Estrategia de Modelado**:\n",
    "- Probar m√∫ltiples algoritmos (Random Forest, XGBoost, LightGBM)\n",
    "- Usar class_weight para manejar desbalanceo\n",
    "- Validaci√≥n cruzada para robustez\n",
    "- Comparar m√©tricas de rendimiento"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calcular pesos de clase para manejar desbalanceo\n",
    "from sklearn.utils.class_weight import compute_class_weight\n",
    "\n",
    "class_weights = compute_class_weight(\n",
    "    class_weight='balanced',\n",
    "    classes=np.unique(y_train),\n",
    "    y=y_train\n",
    ")\n",
    "\n",
    "class_weight_dict = {i: weight for i, weight in enumerate(class_weights)}\n",
    "\n",
    "print(\"=\" * 80)\n",
    "print(\"CONFIGURACI√ìN DE MODELOS\")\n",
    "print(\"=\" * 80)\n",
    "print(f\"\\nPesos de clase (para manejar desbalanceo):\")\n",
    "for class_idx, weight in class_weight_dict.items():\n",
    "    class_name = target_encoder.inverse_transform([class_idx])[0]\n",
    "    print(f\"  Clase {class_idx} ({class_name}): {weight:.3f}\")\n",
    "\n",
    "# Definir modelos\n",
    "models = {\n",
    "    'Random Forest': RandomForestClassifier(\n",
    "        n_estimators=200,\n",
    "        max_depth=20,\n",
    "        min_samples_split=5,\n",
    "        min_samples_leaf=2,\n",
    "        class_weight='balanced',\n",
    "        random_state=42,\n",
    "        n_jobs=-1\n",
    "    ),\n",
    "    'XGBoost': XGBClassifier(\n",
    "        n_estimators=200,\n",
    "        max_depth=8,\n",
    "        learning_rate=0.1,\n",
    "        subsample=0.8,\n",
    "        colsample_bytree=0.8,\n",
    "        random_state=42,\n",
    "        n_jobs=-1,\n",
    "        eval_metric='mlogloss'\n",
    "    ),\n",
    "    'LightGBM': LGBMClassifier(\n",
    "        n_estimators=200,\n",
    "        max_depth=10,\n",
    "        learning_rate=0.1,\n",
    "        num_leaves=31,\n",
    "        subsample=0.8,\n",
    "        colsample_bytree=0.8,\n",
    "        class_weight='balanced',\n",
    "        random_state=42,\n",
    "        n_jobs=-1,\n",
    "        verbose=-1\n",
    "    )\n",
    "}\n",
    "\n",
    "print(f\"\\n‚úì Modelos configurados: {len(models)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5.2 Entrenamiento y Evaluaci√≥n de Modelos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"=\" * 80)\n",
    "print(\"ENTRENAMIENTO DE MODELOS\")\n",
    "print(\"=\" * 80)\n",
    "\n",
    "results = {}\n",
    "trained_models = {}\n",
    "\n",
    "for name, model in models.items():\n",
    "    print(f\"\\n{'='*80}\")\n",
    "    print(f\"Entrenando: {name}\")\n",
    "    print(f\"{'='*80}\")\n",
    "    \n",
    "    # Entrenar modelo\n",
    "    model.fit(X_train, y_train)\n",
    "    trained_models[name] = model\n",
    "    \n",
    "    # Predicciones\n",
    "    y_pred_train = model.predict(X_train)\n",
    "    y_pred_val = model.predict(X_val)\n",
    "    \n",
    "    # M√©tricas\n",
    "    train_accuracy = accuracy_score(y_train, y_pred_train)\n",
    "    val_accuracy = accuracy_score(y_val, y_pred_val)\n",
    "    \n",
    "    print(f\"\\nüìä Resultados:\")\n",
    "    print(f\"  - Accuracy (Train): {train_accuracy:.4f}\")\n",
    "    print(f\"  - Accuracy (Validation): {val_accuracy:.4f}\")\n",
    "    print(f\"  - Diferencia: {abs(train_accuracy - val_accuracy):.4f}\")\n",
    "    \n",
    "    # Reporte de clasificaci√≥n\n",
    "    print(f\"\\nüìã Reporte de Clasificaci√≥n (Validation):\")\n",
    "    print(classification_report(\n",
    "        y_val, \n",
    "        y_pred_val,\n",
    "        target_names=target_encoder.classes_,\n",
    "        digits=4\n",
    "    ))\n",
    "    \n",
    "    # Guardar resultados\n",
    "    results[name] = {\n",
    "        'train_accuracy': train_accuracy,\n",
    "        'val_accuracy': val_accuracy,\n",
    "        'y_pred_val': y_pred_val\n",
    "    }\n",
    "    \n",
    "    # Matriz de confusi√≥n\n",
    "    cm = confusion_matrix(y_val, y_pred_val)\n",
    "    plt.figure(figsize=(8, 6))\n",
    "    sns.heatmap(cm, annot=True, fmt='d', cmap='Blues',\n",
    "                xticklabels=target_encoder.classes_,\n",
    "                yticklabels=target_encoder.classes_)\n",
    "    plt.title(f'Matriz de Confusi√≥n - {name}', fontsize=14, fontweight='bold')\n",
    "    plt.ylabel('Valor Real')\n",
    "    plt.xlabel('Predicci√≥n')\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "print(f\"\\n\\n{'='*80}\")\n",
    "print(\"RESUMEN DE RESULTADOS\")\n",
    "print(f\"{'='*80}\")\n",
    "\n",
    "results_df = pd.DataFrame({\n",
    "    'Modelo': results.keys(),\n",
    "    'Train Accuracy': [r['train_accuracy'] for r in results.values()],\n",
    "    'Validation Accuracy': [r['val_accuracy'] for r in results.values()]\n",
    "})\n",
    "results_df = results_df.sort_values('Validation Accuracy', ascending=False)\n",
    "print(results_df.to_string(index=False))\n",
    "\n",
    "best_model_name = results_df.iloc[0]['Modelo']\n",
    "best_model = trained_models[best_model_name]\n",
    "best_accuracy = results_df.iloc[0]['Validation Accuracy']\n",
    "\n",
    "print(f\"\\nüèÜ Mejor Modelo: {best_model_name}\")\n",
    "print(f\"üìà Accuracy en Validaci√≥n: {best_accuracy:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5.3 An√°lisis de Importancia de Features\n",
    "\n",
    "**Valor de Negocio**: Identificar qu√© factores son m√°s importantes para predecir el estado de las bombas ayuda a:\n",
    "- Priorizar la recolecci√≥n de datos\n",
    "- Enfocar recursos de mantenimiento\n",
    "- Mejorar la toma de decisiones"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Obtener importancia de features del mejor modelo\n",
    "if hasattr(best_model, 'feature_importances_'):\n",
    "    print(\"=\" * 80)\n",
    "    print(f\"IMPORTANCIA DE CARACTER√çSTICAS - {best_model_name}\")\n",
    "    print(\"=\" * 80)\n",
    "    \n",
    "    feature_importance = pd.DataFrame({\n",
    "        'Feature': X.columns,\n",
    "        'Importance': best_model.feature_importances_\n",
    "    }).sort_values('Importance', ascending=False)\n",
    "    \n",
    "    print(\"\\nTop 20 Caracter√≠sticas M√°s Importantes:\")\n",
    "    print(feature_importance.head(20).to_string(index=False))\n",
    "    \n",
    "    # Visualizaci√≥n\n",
    "    plt.figure(figsize=(12, 8))\n",
    "    top_features = feature_importance.head(20)\n",
    "    plt.barh(range(len(top_features)), top_features['Importance'])\n",
    "    plt.yticks(range(len(top_features)), top_features['Feature'])\n",
    "    plt.xlabel('Importancia')\n",
    "    plt.title('Top 20 Caracter√≠sticas M√°s Importantes', fontsize=14, fontweight='bold')\n",
    "    plt.gca().invert_yaxis()\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "    \n",
    "    print(\"\\nüí° Insights de Negocio:\")\n",
    "    print(\"   Las caracter√≠sticas m√°s importantes pueden guiar:\")\n",
    "    print(\"   - Inversiones en infraestructura\")\n",
    "    print(\"   - Protocolos de mantenimiento\")\n",
    "    print(\"   - Recolecci√≥n de datos cr√≠ticos\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Predicciones Finales y Submission\n",
    "\n",
    "### 6.1 Generar Predicciones para el Test Set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"=\" * 80)\n",
    "print(\"GENERANDO PREDICCIONES FINALES\")\n",
    "print(\"=\" * 80)\n",
    "\n",
    "# Reentrenar el mejor modelo con todos los datos de entrenamiento\n",
    "print(f\"\\nReentrenando {best_model_name} con todos los datos de entrenamiento...\")\n",
    "best_model.fit(X, y)\n",
    "\n",
    "# Predicciones en test set\n",
    "y_pred_test = best_model.predict(X_test_final)\n",
    "\n",
    "# Convertir predicciones num√©ricas a labels originales\n",
    "y_pred_test_labels = target_encoder.inverse_transform(y_pred_test)\n",
    "\n",
    "print(f\"\\n‚úì Predicciones generadas: {len(y_pred_test_labels):,}\")\n",
    "print(f\"\\nDistribuci√≥n de predicciones:\")\n",
    "pred_counts = pd.Series(y_pred_test_labels).value_counts()\n",
    "for status, count in pred_counts.items():\n",
    "    percentage = (count / len(y_pred_test_labels)) * 100\n",
    "    print(f\"  - {status}: {count:,} ({percentage:.2f}%)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 6.2 Crear Archivo de Submission"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Crear DataFrame de submission\n",
    "submission = pd.DataFrame({\n",
    "    'id': test_ids,\n",
    "    'status_group': y_pred_test_labels\n",
    "})\n",
    "\n",
    "# Guardar archivo\n",
    "submission_filename = 'submission.csv'\n",
    "submission.to_csv(submission_filename, index=False)\n",
    "\n",
    "print(\"=\" * 80)\n",
    "print(\"ARCHIVO DE SUBMISSION CREADO\")\n",
    "print(\"=\" * 80)\n",
    "print(f\"\\n‚úì Archivo guardado como: {submission_filename}\")\n",
    "print(f\"\\nPrimeras filas del archivo de submission:\")\n",
    "print(submission.head(10))\n",
    "print(f\"\\n√öltimas filas del archivo de submission:\")\n",
    "print(submission.tail(10))\n",
    "\n",
    "print(f\"\\n\\n{'='*80}\")\n",
    "print(\"INSTRUCCIONES PARA SUBIR LA SUBMISSION\")\n",
    "print(f\"{'='*80}\")\n",
    "print(\"\\n1. Ir a: https://www.drivendata.org/competitions/7/pump-it-up-data-mining-the-water-table/submissions/\")\n",
    "print(\"2. Hacer clic en 'Submit Predictions'\")\n",
    "print(f\"3. Subir el archivo: {submission_filename}\")\n",
    "print(\"4. El score aparecer√° en el leaderboard\")\n",
    "print(\"\\nüí° El score oficial ser√° la m√©trica 'Classification Rate' en el leaderboard\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Conclusiones y Recomendaciones de Negocio\n",
    "\n",
    "### 7.1 Resumen Ejecutivo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"=\" * 80)\n",
    "print(\"RESUMEN EJECUTIVO DEL PROYECTO\")\n",
    "print(\"=\" * 80)\n",
    "\n",
    "print(f\"\\nüìä M√âTRICAS DEL MODELO\")\n",
    "print(f\"{'-'*80}\")\n",
    "print(f\"Mejor Modelo: {best_model_name}\")\n",
    "print(f\"Accuracy en Validaci√≥n: {best_accuracy:.2%}\")\n",
    "print(f\"Total de caracter√≠sticas usadas: {X.shape[1]}\")\n",
    "print(f\"Total de bombas en entrenamiento: {len(train_data):,}\")\n",
    "print(f\"Total de predicciones generadas: {len(submission):,}\")\n",
    "\n",
    "print(f\"\\nüéØ DISTRIBUCI√ìN DE CLASES EN PREDICCIONES\")\n",
    "print(f\"{'-'*80}\")\n",
    "for status, count in pred_counts.items():\n",
    "    percentage = (count / len(y_pred_test_labels)) * 100\n",
    "    print(f\"{status:30s}: {count:6,} ({percentage:5.2f}%)\")\n",
    "\n",
    "print(f\"\\nüí° INSIGHTS CLAVE PARA EL NEGOCIO\")\n",
    "print(f\"{'-'*80}\")\n",
    "print(\"\\n1. CALIDAD DE LOS DATOS\")\n",
    "print(\"   - Se detectaron valores faltantes y ceros en variables clave (coordenadas, a√±o)\")\n",
    "print(\"   - Recomendaci√≥n: Mejorar procesos de recolecci√≥n de datos en campo\")\n",
    "\n",
    "print(\"\\n2. FACTORES CR√çTICOS DE FALLA\")\n",
    "print(\"   - Las caracter√≠sticas m√°s importantes incluyen:\")\n",
    "if hasattr(best_model, 'feature_importances_'):\n",
    "    top_3_features = feature_importance.head(3)['Feature'].tolist()\n",
    "    for feat in top_3_features:\n",
    "        print(f\"     ‚Ä¢ {feat}\")\n",
    "print(\"   - Recomendaci√≥n: Monitorear estos factores proactivamente\")\n",
    "\n",
    "print(\"\\n3. DESBALANCEO DE CLASES\")\n",
    "print(\"   - 'functional needs repair' es la clase minoritaria\")\n",
    "print(\"   - Recomendaci√≥n: Establecer alertas tempranas para bombas en esta categor√≠a\")\n",
    "\n",
    "print(\"\\n4. OPORTUNIDADES DE MEJORA\")\n",
    "print(\"   - Recolectar m√°s datos de bombas que necesitan reparaci√≥n\")\n",
    "print(\"   - Implementar sistema de monitoreo en tiempo real\")\n",
    "print(\"   - Desarrollar estrategia de mantenimiento preventivo basada en predicciones\")\n",
    "\n",
    "print(f\"\\n\\n{'='*80}\")\n",
    "print(\"PR√ìXIMOS PASOS\")\n",
    "print(f\"{'='*80}\")\n",
    "print(\"\\n1. ‚úÖ Subir submission.csv al concurso de DrivenData\")\n",
    "print(\"2. üìù Registrar el score oficial obtenido\")\n",
    "print(\"3. üîÑ Iterar mejorando el modelo si es necesario:\")\n",
    "print(\"   - Probar feature engineering adicional\")\n",
    "print(\"   - Optimizar hiperpar√°metros con GridSearch/RandomSearch\")\n",
    "print(\"   - Considerar ensemble de modelos\")\n",
    "print(\"4. üöÄ Implementar modelo en producci√≥n\")\n",
    "print(\"5. üìä Establecer m√©tricas de negocio para monitoreo continuo\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 7.2 Registro del Score del Concurso\n",
    "\n",
    "**IMPORTANTE**: Despu√©s de subir la submission al concurso, registra aqu√≠ el score oficial obtenido."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# COMPLETAR DESPU√âS DE SUBIR AL CONCURSO\n",
    "print(\"=\" * 80)\n",
    "print(\"SCORE OFICIAL DEL CONCURSO\")\n",
    "print(\"=\" * 80)\n",
    "print(\"\\n‚ö†Ô∏è  INSTRUCCIONES:\")\n",
    "print(\"   1. Sube el archivo 'submission.csv' al concurso\")\n",
    "print(\"   2. Anota el score (Classification Rate) que aparece en el leaderboard\")\n",
    "print(\"   3. Actualiza la variable 'competition_score' a continuaci√≥n\")\n",
    "print(\"\\n\" + \"-\"*80)\n",
    "\n",
    "# ACTUALIZAR ESTA VARIABLE CON EL SCORE REAL\n",
    "competition_score = None  # Ejemplo: 0.8234\n",
    "\n",
    "if competition_score is not None:\n",
    "    print(f\"\\nüèÜ Score Oficial del Concurso: {competition_score:.4f}\")\n",
    "    print(f\"   Accuracy en Validaci√≥n Local: {best_accuracy:.4f}\")\n",
    "    print(f\"   Diferencia: {abs(competition_score - best_accuracy):.4f}\")\n",
    "    \n",
    "    if abs(competition_score - best_accuracy) < 0.02:\n",
    "        print(\"\\n‚úÖ El modelo generaliza bien - scores consistentes\")\n",
    "    else:\n",
    "        print(\"\\n‚ö†Ô∏è  Diferencia significativa - posible overfitting o diferencias en datos\")\nelse:\n",
    "    print(\"\\n‚ùå Score a√∫n no registrado\")\n",
    "    print(\"   Por favor, sube la submission y actualiza la variable 'competition_score'\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8. Mejoras Futuras (Opcional)\n",
    "\n",
    "Si el score inicial no es satisfactorio, considera estas mejoras:\n",
    "\n",
    "### 8.1 Feature Engineering Avanzado\n",
    "- Crear interacciones entre features importantes\n",
    "- Agregar features geoespaciales m√°s complejas\n",
    "- Aplicar transformaciones no lineales\n",
    "\n",
    "### 8.2 Optimizaci√≥n de Hiperpar√°metros\n",
    "- GridSearchCV o RandomizedSearchCV\n",
    "- Bayesian Optimization\n",
    "- Validaci√≥n cruzada m√°s robusta\n",
    "\n",
    "### 8.3 Ensemble Methods\n",
    "- Voting Classifier combinando m√∫ltiples modelos\n",
    "- Stacking\n",
    "- Blending\n",
    "\n",
    "### 8.4 T√©cnicas de Balanceo M√°s Sofisticadas\n",
    "- ADASYN\n",
    "- Diferentes estrategias de SMOTE\n",
    "- Cost-sensitive learning"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Fin del Notebook\n",
    "\n",
    "**Autor**: [Tu Nombre]\n",
    "\n",
    "**Fecha**: [Fecha]\n",
    "\n",
    "**Competici√≥n**: Pump it Up - DrivenData.org\n",
    "\n",
    "---"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
